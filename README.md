# arxiv-daily latest papers around heart rate arxiv paper
Automated deployment @ 2023-06-09 21:28:54 Asia/Shanghai
> Welcome to contribute! Add your topics and keywords in [`topic.yml`]({repo_url}/blob/main/database/topic.yml).
> You can also view historical data through the [storage]({repo_url}/blob/main/database/storage).

## HRV

### hrv
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**Inequalities on Tent Spaces and Closed Range Integration Operators on spaces of Average Radial Integrability**|TanausÚ Aguilar-HernÁndez et.al.|[2306.05408v1](http://arxiv.org/abs/2306.05408v1)|null|We deal with a reverse Carleson measure inequality for the tent spaces of analytic functions in the unit disc $\mathbb{D}$ of the complex plane. The tent spaces of measurable functions were introduced by Coifman, Meyer and Stein.   Let $1\leq p,q < \infty$ and consider the positive Borel measure $d\mu(z) = \chi_{G}(z)\frac{dm(z)}{(1-|z|)}$ defined in terms of a measurable set $G \subseteq \mathbb{D}$ and of the area Lebesgue measure $dm(z)$ in $\mathbb{D}$. We prove a necessary and sufficient condition on $G$ in order to exist a constant $K>0$ such that $$   \int_{\mathbb{T}} \left(\int_{\Gamma(\xi)} |f(z)|^{p}\ d\mu(z) \right)^{q/p}\ |d\xi|\geq K \,\int_{\mathbb{T}} \left(\int_{\Gamma(\xi)} |f(z)|^{p}\ \frac{dm(z)}{1-|z|}\right)^{q/p}\ |d\xi|, $$ for any $f$ analytic in $\mathbb{D}$ with the property, the right term of the inequality above is finite. Here $\mathbb{T}$ stands for the unit circle and $\Gamma(\xi)$ is a non-tangential region with vertex at $\xi \in \mathbb{T}$.   This work extends the study of D. Luecking on Bergman spaces to the analytic tent spaces. We apply our result to characterize the closed range property of the integration operator known as Pommerenke operator when acting on the average radial integrability spaces. T. Aguilar-Hern\'andez, M. Contreras and L. Rodr\'iguez-Piazza introduced these spaces for the first time in the literature. The Hardy and the Bergman spaces form part of this family.|
|**2023-06-08**|**Classical simulations of noisy variational quantum circuits**|Enrico Fontana et.al.|[2306.05400v1](http://arxiv.org/abs/2306.05400v1)|null|Noise detrimentally affects quantum computations so that they not only become less accurate but also easier to simulate classically as systems scale up. We construct a classical simulation algorithm, LOWESA (low weight efficient simulation algorithm), for estimating expectation values of noisy parameterised quantum circuits. It combines previous results on spectral analysis of parameterised circuits with Pauli back-propagation and recent ideas for simulations of noisy random circuits. We show, under some conditions on the circuits and mild assumptions on the noise, that LOWESA gives an efficient, polynomial algorithm in the number of qubits (and depth), with approximation error that vanishes exponentially in the physical error rate and a controllable cut-off parameter. We also discuss the practical limitations of the method for circuit classes with correlated parameters and its scaling with decreasing error rates.|
|**2023-06-08**|**HQ-50K: A Large-scale, High-quality Dataset for Image Restoration**|Qinhong Yang et.al.|[2306.05390v1](http://arxiv.org/abs/2306.05390v1)|[link](https://github.com/littleyaang/hq-50k)|This paper introduces a new large-scale image restoration dataset, called HQ-50K, which contains 50,000 high-quality images with rich texture details and semantic diversity. We analyze existing image restoration datasets from five different perspectives, including data scale, resolution, compression rates, texture details, and semantic coverage. However, we find that all of these datasets are deficient in some aspects. In contrast, HQ-50K considers all of these five aspects during the data curation process and meets all requirements. We also present a new Degradation-Aware Mixture of Expert (DAMoE) model, which enables a single model to handle multiple corruption types and unknown levels. Our extensive experiments demonstrate that HQ-50K consistently improves the performance on various image restoration tasks, such as super-resolution, denoising, dejpeg, and deraining. Furthermore, our proposed DAMoE, trained on our \dataset, outperforms existing state-of-the-art unified models designed for multiple restoration tasks and levels. The dataset and code are available at \url{https://github.com/littleYaang/HQ-50K}.|
|**2023-06-08**|**Utterance Emotion Dynamics in Children's Poems: Emotional Changes Across Age**|Daniela Teodorescu et.al.|[2306.05387v1](http://arxiv.org/abs/2306.05387v1)|null|Emerging psychopathology studies are showing that patterns of changes in emotional state -- emotion dynamics -- are associated with overall well-being and mental health. More recently, there has been some work in tracking emotion dynamics through one's utterances, allowing for data to be collected on a larger scale across time and people. However, several questions about how emotion dynamics change with age, especially in children, and when determined through children's writing, remain unanswered. In this work, we use both a lexicon and a machine learning based approach to quantify characteristics of emotion dynamics determined from poems written by children of various ages. We show that both approaches point to similar trends: consistent increasing intensities for some emotions (e.g., anger, fear, joy, sadness, arousal, and dominance) with age and a consistent decreasing valence with age. We also find increasing emotional variability, rise rates (i.e., emotional reactivity), and recovery rates (i.e., emotional regulation) with age. These results act as a useful baselines for further research in how patterns of emotions expressed by children change with age, and their association with mental health.|
|**2023-06-08**|**Rate Forecaster based Energy Aware Band Assignment in Multiband Networks**|Brijesh Soni et.al.|[2306.05369v1](http://arxiv.org/abs/2306.05369v1)|null|The high frequency communication bands (mmWave and sub-THz) promise tremendous data rates, however, they also have very high power consumption which is particularly significant for battery-power-limited user-equipment (UE). In this context, we design an energy aware band assignment system which reduces the power consumption while also achieving a target sum rate of M in T time-slots. We do this by using 1) Rate forecaster(s); 2) Channel forecaster(s) which forecasts T direct multistep ahead using a stacked (long short term memory) LSTM architecture. We propose an iterative rate updating algorithm which updates the target rate based on current rate and future predicted rates in a frame. The proposed approach is validated on the publicly available `DeepMIMO' dataset. Research findings shows that the rate forecaster based approach performs better than the channel forecaster. Furthermore, LSTM based predictions outperforms well celebrated Transformer predictions in terms of NRMSE and NMAE. Research findings reveals that the power consumption with this approach is ~ 300 mW lower compared to a greedy band assignment at a 1.5Gb/s target rate.|
|**2023-06-08**|**Ordinal Potential-based Player Rating**|Nelson Vadori et.al.|[2306.05366v1](http://arxiv.org/abs/2306.05366v1)|null|A two-player symmetric zero-sum game is transitive if for any pure strategies $x$, $y$, $z$, if $x$ is better than $y$, and $y$ is better than $z$, then $x$ is better than $z$. It was recently observed that the Elo rating fails at preserving transitive relations among strategies and therefore cannot correctly extract the transitive component of a game. Our first contribution is to show that the Elo rating actually does preserve transitivity when computed in the right space. Precisely, using a suitable invertible mapping $\varphi$, we first apply $\varphi$ to the game, then compute Elo ratings, then go back to the original space by applying $\varphi^{-1}$. We provide a characterization of transitive games as a weak variant of ordinal potential games with additively separable potential functions. Leveraging this insight, we introduce the concept of transitivity order, the minimum number of invertible mappings required to transform the payoff of a transitive game into (differences of) its potential function. The transitivity order is a tool to classify transitive games, with Elo games being an example of transitive games of order one. Most real-world games have both transitive and non-transitive (cyclic) components, and we use our analysis of transitivity to extract the transitive (potential) component of an arbitrary game. We link transitivity to the known concept of sign-rank: transitive games have sign-rank two; arbitrary games may have higher sign-rank. Using a neural network-based architecture, we learn a decomposition of an arbitrary game into transitive and cyclic components that prioritises capturing the sign pattern of the game. In particular, a transitive game always has just one component in its decomposition, the potential component. We provide a comprehensive evaluation of our methodology using both toy examples and empirical data from real-world games.|
|**2023-06-08**|**Surrogate method for partial association between mixed data with application to well-being survey analysis**|Shaobo Li et.al.|[2306.05362v1](http://arxiv.org/abs/2306.05362v1)|null|This paper is motivated by the analysis of a survey study of college student wellbeing before and after the outbreak of the COVID-19 pandemic. A statistical challenge in well-being survey studies lies in that outcome variables are often recorded in different scales, be it continuous, binary, or ordinal. The presence of mixed data complicates the assessment of the associations between them while adjusting for covariates. In our study, of particular interest are the associations between college students' wellbeing and other mental health measures and how other risk factors moderate these associations during the pandemic. To this end, we propose a unifying framework for studying partial association between mixed data. This is achieved by defining a unified residual using the surrogate method. The idea is to map the residual randomness to the same continuous scale, regardless of the original scales of outcome variables. It applies to virtually all commonly used models for covariate adjustments. We demonstrate the validity of using such defined residuals to assess partial association. In particular, we develop a measure that generalizes classical Kendall's tau in the sense that it can size both partial and marginal associations. More importantly, our development advances the theory of the surrogate method developed in recent years by showing that it can be used without requiring outcome variables having a latent variable structure. The use of our method in the well-being survey analysis reveals (i) significant moderation effects (i.e., the difference between partial and marginal associations) of some key risk factors; and (ii) an elevated moderation effect of physical health, loneliness, and accommodation after the onset of COVID-19.|
|**2023-06-08**|**A Data-Driven Approach to Positioning Grab Bars in the Sagittal Plane for Elderly Persons**|Roberto Bolli Jr. et.al.|[2306.05343v1](http://arxiv.org/abs/2306.05343v1)|null|The placement of grab bars for elderly users is based largely on ADA building codes and does not reflect the large differences in height, mobility, and muscle power between individual persons. The goal of this study is to see if there are any correlations between an elderly user's preferred handlebar pose and various demographic indicators, self-rated mobility for tasks requiring postural change, and biomechanical markers. For simplicity, we consider only the case where the handlebar is positioned directly in front of the user, as this confines the relevant body kinematics to a 2D sagittal plane. Previous eldercare devices have been constructed to position a handlebar in various poses in space. Our work augments these devices and adds to the body of knowledge by assessing how the handlebar should be positioned based on data on actual elderly people instead of simulations.|
|**2023-06-08**|**Spontaneous Self-Constraint in Active Nematic Flows**|Louise C. Head et.al.|[2306.05328v1](http://arxiv.org/abs/2306.05328v1)|null|Active processes drive and guide biological dynamics across scales -- from subcellular cytoskeletal remodelling, through tissue development in embryogenesis, to population-level bacterial colonies expansion. In each of these, biological functionality requires collective flows to occur while self-organized structures are protected; however, the mechanisms by which active flows can spontaneously constrain their dynamics to preserve structure have not previously been explained. By studying collective flows and defect dynamics in active nematic films, we demonstrate the existence of a self-constraint -- a two-way, spontaneously arising relationship between activity-driven isosurfaces of flow boundaries and mesoscale nematic structures. Our results show that self-motile defects are tightly constrained to viscometric surfaces -- contours along which vorticity and strain-rate balance. This in turn reveals that self-motile defects break mirror symmetry when they move along a single viscometric surface, in contrast with expectations. This is explained by an interdependence between viscometric surfaces and bend walls -- elongated narrow kinks in the orientation field. Although we focus on extensile nematic films, numerical results show the constraint holds whenever activity leads to motile half-charge defects. This mesoscale cross-field self-constraint offers a new framework for tackling complex 3D active turbulence, designing dynamic control into biomimetic materials, and understanding how biological systems can employ active stress for dynamic self-organization.|
|**2023-06-08**|**Perching by hugging: an initial feasibility study**|William Stewart et.al.|[2306.05324v1](http://arxiv.org/abs/2306.05324v1)|null|Current UAVs capable of perching require added structure and mechanisms to accomplish this. These take the form of hooks, claws, needles, etc which add weight and usually drag. We propose in this paper the dual use of structures already on the vehicle to enable perching, thus reducing the weight and drag cost associated with perching UAVs. We propose a wing design capable of passively wrapping around a vertical pole to perch. We experimentally investigate the feasibility of the design, presenting results on minimum required perching speeds as well as the effect of weight distribution on the success rate of the wing wrapping. Finally, we comment on design requirements for holding onto the pole based on our findings.|
|**2023-06-08**|**Real-time whole-heart electromechanical simulations using Latent Neural Ordinary Differential Equations**|Matteo Salvador et.al.|[2306.05321v1](http://arxiv.org/abs/2306.05321v1)|null|Cardiac digital twins provide a physics and physiology informed framework to deliver predictive and personalized medicine. However, high-fidelity multi-scale cardiac models remain a barrier to adoption due to their extensive computational costs and the high number of model evaluations needed for patient-specific personalization. Artificial Intelligence-based methods can make the creation of fast and accurate whole-heart digital twins feasible. In this work, we use Latent Neural Ordinary Differential Equations (LNODEs) to learn the temporal pressure-volume dynamics of a heart failure patient. Our surrogate model based on LNODEs is trained from 400 3D-0D whole-heart closed-loop electromechanical simulations while accounting for 43 model parameters, describing single cell through to whole organ and cardiovascular hemodynamics. The trained LNODEs provides a compact and efficient representation of the 3D-0D model in a latent space by means of a feedforward fully-connected Artificial Neural Network that retains 3 hidden layers with 13 neurons per layer and allows for 300x real-time numerical simulations of the cardiac function on a single processor of a standard laptop. This surrogate model is employed to perform global sensitivity analysis and robust parameter estimation with uncertainty quantification in 3 hours of computations, still on a single processor. We match pressure and volume time traces unseen by the LNODEs during the training phase and we calibrate 4 to 11 model parameters while also providing their posterior distribution. This paper introduces the most advanced surrogate model of cardiac function available in the literature and opens new important venues for parameter calibration in cardiac digital twins.|
|**2023-06-08**|**Large-scale adaptive multiple testing for sequential data controlling false discovery and nondiscovery rates**|Rahul Roy et.al.|[2306.05315v1](http://arxiv.org/abs/2306.05315v1)|null|In modern scientific experiments, we frequently encounter data that have large dimensions, and in some experiments, such high dimensional data arrive sequentially rather than full data being available all at a time. We develop multiple testing procedures with simultaneous control of false discovery and nondiscovery rates when $m$-variate data vectors $\mathbf{X}_1, \mathbf{X}_2, \dots$ are observed sequentially or in groups and each coordinate of these vectors leads to a hypothesis testing. Existing multiple testing methods for sequential data uses fixed stopping boundaries that do not depend on sample size, and hence, are quite conservative when the number of hypotheses $m$ is large. We propose sequential tests based on adaptive stopping boundaries that ensure shrinkage of the continue sampling region as the sample size increases. Under minimal assumptions on the data sequence, we first develop a test based on an oracle test statistic such that both false discovery rate (FDR) and false nondiscovery rate (FNR) are nearly equal to some prefixed levels with strong control. Under a two-group mixture model assumption, we propose a data-driven stopping and decision rule based on local false discovery rate statistic that mimics the oracle rule and guarantees simultaneous control of FDR and FNR asymptotically as $m$ tends to infinity. Both the oracle and the data-driven stopping times are shown to be finite (i.e., proper) with probability 1 for all finite $m$ and converge to a finite constant as $m$ grows to infinity. Further, we compare the data-driven test with the existing gap rule proposed in He and Bartroff (2021) and show that the ratio of the expected sample sizes of our method and the gap rule tends to zero as $m$ goes to infinity. Extensive analysis of simulated datasets as well as some real datasets illustrate the superiority of the proposed tests over some existing methods.|
|**2023-06-08**|**Mode-locked laser in nanophotonic lithium niobate**|Qiushi Guo et.al.|[2306.05314v1](http://arxiv.org/abs/2306.05314v1)|null|Mode-locked lasers (MLLs) have enabled ultrafast sciences and technologies by generating ultrashort pulses with peak powers substantially exceeding their average powers. Recently, tremendous efforts have been focused on realizing integrated MLLs not only to address the challenges associated with their size and power demand, but also to enable transforming the ultrafast technologies into nanophotonic chips, and ultimately to unlock their potential for a plethora of applications. However, till now the prospect of integrated MLLs driving ultrafast nanophotonic circuits has remained elusive because of their typically low peak powers, lack of controllability, and challenges with integration with appropriate nanophotonic platforms. Here, we overcome these limitations by demonstrating an electrically-pumped actively MLL in nanophotonic lithium niobate based on its hybrid integration with a III-V semiconductor optical amplifier. Our MLL generates $\sim$4.8 ps optical pulses around 1065 nm at a repetition rate of $\sim$10 GHz, with pulse energy exceeding 2.6 pJ and a high peak power beyond 0.5 W. We show that both the repetition rate and the carrier-envelope-offset of the resulting frequency comb can be flexibly controlled in a wide range using the RF driving frequency and the pump current, paving the way for fully-stabilized on-chip frequency combs in nanophotonics. Our work marks an important step toward fully-integrated nonlinear and ultrafast photonic systems in nanophotonic lithium niobate.|
|**2023-06-08**|**Emergent circulation patterns from anonymized mobility data: Clustering Italy in the time of Covid**|Jules Morand et.al.|[2306.05302v1](http://arxiv.org/abs/2306.05302v1)|null|Using anonymized mobility data from Facebook users and publicly available information on the Italian population, we model the circulation of people in Italy before and during the early phase of the SARS-CoV-2 pandemic (COVID-19). We perform a spatial and temporal clustering of the movement network at the level of fluxes across provinces on a daily basis. The resulting partition in time successfully identifies the first two lockdowns without any prior information. Similarly, the spatial clustering returns 11 to 23 clusters depending on the period ("standard" mobility vs. lockdown) using the greedy modularity communities clustering method, and 16 to 30 clusters using the critical variable selection method. Fascinatingly, the spatial clusters obtained with both methods are strongly reminiscent of the 11 regions into which emperor Augustus had divided Italy according to Pliny the Elder. This work introduces and validates a data analysis pipeline that enables us: i) to assess the reliability of data obtained from a partial and potentially biased sample of the population in performing estimates of population mobility nationwide; ii) to identify areas of a Country with well-defined mobility patterns, and iii) to distinguish different patterns from one another, resolve them in time and find their optimal spatial extent. The proposed method is generic and can be applied to other countries, with different geographical scales, and also to similar networks (e.g. biological networks). The results can thus represent a relevant step forward in the development of methods and strategies for the containment of future epidemic phenomena.|
|**2023-06-08**|**Chiral EFT calculation of neutrino reactions in warm neutron-rich matter**|Eunkyoung Shin et.al.|[2306.05280v1](http://arxiv.org/abs/2306.05280v1)|null|Neutrino scattering and absorption rates of relevance to supernovae and neutron star mergers are obtained from nuclear matter dynamical structure functions that encode many-body effects from nuclear mean fields and correlations. We employ nuclear interactions from chiral effective field theory to calculate the density, spin, isospin, and spin-isospin response functions of warm beta-equilibrium nuclear matter. We include corrections to the single-particle energies in the mean field approximation as well as vertex corrections resummed in the random phase approximation (RPA), including, for the first time, both direct and exchange diagrams. We find that correlations included through the RPA redistribute the strength of the response to higher energy for neutrino absorption and lower energy for antineutrino absorption. This tends to suppress the absorption rate of electron neutrinos across all relevant energy scales. In contrast, the inclusion of RPA correlations enhances the electron antineutrino absorption rate at low energy and supresses the rate at high energy. These effects are especially important at high-density and in the vicinity of the neutrino decoupling region. Implications for heavy element nucleosynthesis, electromagnetic signatures of compact object mergers, supernova dynamics, and neutrino detection from galactic supernovae are discussed briefly.|
|**2023-06-08**|**Language-specific Acoustic Boundary Learning for Mandarin-English Code-switching Speech Recognition**|Zhiyun Fan et.al.|[2306.05279v1](http://arxiv.org/abs/2306.05279v1)|null|Code-switching speech recognition (CSSR) transcribes speech that switches between multiple languages or dialects within a single sentence. The main challenge in this task is that different languages often have similar pronunciations, making it difficult for models to distinguish between them. In this paper, we propose a method for solving the CSSR task from the perspective of language-specific acoustic boundary learning. We introduce language-specific weight estimators (LSWE) to model acoustic boundary learning in different languages separately. Additionally, a non-autoregressive (NAR) decoder and a language change detection (LCD) module are employed to assist in training. Evaluated on the SEAME corpus, our method achieves a state-of-the-art mixed error rate (MER) of 16.29% and 22.81% on the test_man and test_sge sets. We also demonstrate the effectiveness of our method on a 9000-hour in-house meeting code-switching dataset, where our method achieves a relatively 7.9% MER reduction.|
|**2023-06-08**|**Large deviations of return times and related entropy estimators on shift spaces**|Noé Cuneo et.al.|[2306.05277v1](http://arxiv.org/abs/2306.05277v1)|null|We prove the large deviation principle for several entropy and cross entropy estimators based on return times and waiting times on shift spaces over finite alphabets. In the case of standard return times, we obtain a nonconvex large-deviation rate function. We consider shift-invariant probability measures satisfying some decoupling conditions which imply no form of mixing nor ergodicity. We establish precise relations between the rate functions of the different estimators, and between these rate functions and the corresponding pressures, one of which is the R\'enyi entropy function. The results apply in particular to irreducible Markov chains, equilibrium measures for Bowen-regular potentials, g-measures, invariant Gibbs states for summable interactions in statistical mechanics, and also to probability measures that may be far from Gibbsian, including some hidden Markov models and repeated quantum measurement processes.|
|**2023-06-08**|**Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models**|Tianzhe Chu et.al.|[2306.05272v1](http://arxiv.org/abs/2306.05272v1)|[link](https://github.com/leslietrue/cpp)|The advent of large pre-trained models has brought about a paradigm shift in both visual representation learning and natural language processing. However, clustering unlabeled images, as a fundamental and classic machine learning problem, still lacks effective solution, particularly for large-scale datasets. In this paper, we propose a novel image clustering pipeline that leverages the powerful feature representation of large pre-trained models such as CLIP and cluster images effectively and efficiently at scale. We show that the pre-trained features are significantly more structured by further optimizing the rate reduction objective. The resulting features may significantly improve the clustering accuracy, e.g., from 57\% to 66\% on ImageNet-1k. Furthermore, by leveraging CLIP's image-text binding, we show how the new clustering method leads to a simple yet effective self-labeling algorithm that successfully works on unlabeled large datasets such as MS-COCO and LAION-Aesthetics. We will release the code in https://github.com/LeslieTrue/CPP.|
|**2023-06-08**|**Unscented Autoencoder**|Faris Janjoš et.al.|[2306.05256v1](http://arxiv.org/abs/2306.05256v1)|null|The Variational Autoencoder (VAE) is a seminal approach in deep generative modeling with latent variables. Interpreting its reconstruction process as a nonlinear transformation of samples from the latent posterior distribution, we apply the Unscented Transform (UT) -- a well-known distribution approximation used in the Unscented Kalman Filter (UKF) from the field of filtering. A finite set of statistics called sigma points, sampled deterministically, provides a more informative and lower-variance posterior representation than the ubiquitous noise-scaling of the reparameterization trick, while ensuring higher-quality reconstruction. We further boost the performance by replacing the Kullback-Leibler (KL) divergence with the Wasserstein distribution metric that allows for a sharper posterior. Inspired by the two components, we derive a novel, deterministic-sampling flavor of the VAE, the Unscented Autoencoder (UAE), trained purely with regularization-like terms on the per-sample posterior. We empirically show competitive performance in Fr\'echet Inception Distance (FID) scores over closely-related models, in addition to a lower training variance than the VAE.|
|**2023-06-08**|**Toward more accurate and generalizable brain deformation estimators for traumatic brain injury detection with unsupervised domain adaptation**|Xianghao Zhan et.al.|[2306.05255v1](http://arxiv.org/abs/2306.05255v1)|[link](https://github.com/xzhan96-stf/drca-mlhm)|Machine learning head models (MLHMs) are developed to estimate brain deformation for early detection of traumatic brain injury (TBI). However, the overfitting to simulated impacts and the lack of generalizability caused by distributional shift of different head impact datasets hinders the broad clinical applications of current MLHMs. We propose brain deformation estimators that integrates unsupervised domain adaptation with a deep neural network to predict whole-brain maximum principal strain (MPS) and MPS rate (MPSR). With 12,780 simulated head impacts, we performed unsupervised domain adaptation on on-field head impacts from 302 college football (CF) impacts and 457 mixed martial arts (MMA) impacts using domain regularized component analysis (DRCA) and cycle-GAN-based methods. The new model improved the MPS/MPSR estimation accuracy, with the DRCA method significantly outperforming other domain adaptation methods in prediction accuracy (p<0.001): MPS RMSE: 0.027 (CF) and 0.037 (MMA); MPSR RMSE: 7.159 (CF) and 13.022 (MMA). On another two hold-out test sets with 195 college football impacts and 260 boxing impacts, the DRCA model significantly outperformed the baseline model without domain adaptation in MPS and MPSR estimation accuracy (p<0.001). The DRCA domain adaptation reduces the MPS/MPSR estimation error to be well below TBI thresholds, enabling accurate brain deformation estimation to detect TBI in future clinical applications.|
|**2023-06-08**|**Matching Latent Encoding for Audio-Text based Keyword Spotting**|Kumari Nishu et.al.|[2306.05245v1](http://arxiv.org/abs/2306.05245v1)|null|Using audio and text embeddings jointly for Keyword Spotting (KWS) has shown high-quality results, but the key challenge of how to semantically align two embeddings for multi-word keywords of different sequence lengths remains largely unsolved. In this paper, we propose an audio-text-based end-to-end model architecture for flexible keyword spotting (KWS), which builds upon learned audio and text embeddings. Our architecture uses a novel dynamic programming-based algorithm, Dynamic Sequence Partitioning (DSP), to optimally partition the audio sequence into the same length as the word-based text sequence using the monotonic alignment of spoken content. Our proposed model consists of an encoder block to get audio and text embeddings, a projector block to project individual embeddings to a common latent space, and an audio-text aligner containing a novel DSP algorithm, which aligns the audio and text embeddings to determine if the spoken content is the same as the text. Experimental results show that our DSP is more effective than other partitioning schemes, and the proposed architecture outperformed the state-of-the-art results on the public dataset in terms of Area Under the ROC Curve (AUC) and Equal-Error-Rate (EER) by 14.4 % and 28.9%, respectively.|
|**2023-06-08**|**Boosting-based Construction of BDDs for Linear Threshold Functions and Its Application to Verification of Neural Networks**|Yiping Tang et.al.|[2306.05211v1](http://arxiv.org/abs/2306.05211v1)|null|Understanding the characteristics of neural networks is important but difficult due to their complex structures and behaviors. Some previous work proposes to transform neural networks into equivalent Boolean expressions and apply verification techniques for characteristics of interest. This approach is promising since rich results of verification techniques for circuits and other Boolean expressions can be readily applied. The bottleneck is the time complexity of the transformation. More precisely, (i) each neuron of the network, i.e., a linear threshold function, is converted to a Binary Decision Diagram (BDD), and (ii) they are further combined into some final form, such as Boolean circuits. For a linear threshold function with $n$ variables, an existing method takes $O(n2^{\frac{n}{2}})$ time to construct an ordered BDD of size $O(2^{\frac{n}{2}})$ consistent with some variable ordering. However, it is non-trivial to choose a variable ordering producing a small BDD among $n!$ candidates.   We propose a method to convert a linear threshold function to a specific form of a BDD based on the boosting approach in the machine learning literature. Our method takes $O(2^n \text{poly}(1/\rho))$ time and outputs BDD of size $O(\frac{n^2}{\rho^4}\ln{\frac{1}{\rho}})$, where $\rho$ is the margin of some consistent linear threshold function. Our method does not need to search for good variable orderings and produces a smaller expression when the margin of the linear threshold function is large. More precisely, our method is based on our new boosting algorithm, which is of independent interest. We also propose a method to combine them into the final Boolean expression representing the neural network.|
|**2023-06-08**|**The Attractor Flow for AdS5 Black Holes in $\mathcal{N} = 2$ Gauged Supergravity**|Marina David et.al.|[2306.05206v1](http://arxiv.org/abs/2306.05206v1)|null|We study the flow equations for BPS black holes in $\mathcal{N} = 2$ five-dimensional gauged supergravity coupled to any number of vector multiplets via FI couplings. We develop the Noether-Wald procedure in this context and exhibit the conserved charges as explicit integrals of motion, in the sense that they can be computed at any radius on the rotating spacetime. The boundary conditions needed to solve the first order differential equations are discussed in great detail. We extremize the entropy function that controls the near horizon geometry and give explicit formulae for all geometric variables at their supersymmetric extrema. We have also considered a complexification of the near-horizon variables that elucidates some features of the theory from the near-horizon perspective.|
|**2023-06-08**|**Precision Measurements of $D_s^+ \to ηe^+ ν_e$ and $D_s^+ \to η^\prime e^+ ν_e$**|BESIII Collaboration et.al.|[2306.05194v1](http://arxiv.org/abs/2306.05194v1)|null|Precision measurements of the semileptonic decays $D_s^+ \to \eta e^+ \nu_e$ and $D_s^+ \to \eta^\prime e^+ \nu_e$ are performed using 7.33\,fb$^{-1}$ of $e^+e^-$ collision data collected at center-of-mass energies between 4.128 and 4.226 GeV with the BESIII detector. The branching fractions obtained are $\mathcal{B}(D_s^+ \to \eta e^{+} \nu_e)$ = $(2.251\pm0.039_{\rm stat.}\pm 0.051_{\rm syst.})\%$ and $\mathcal{B}(D_s^+ \to \eta^{\prime} e^{+} \nu_e)$ = $(0.810\pm0.038_{\rm stat.}\pm 0.024_{\rm syst.})\%$. Combining these results with the $\mathcal{B}(D^+\to\eta e^+ \nu_e)$ and $\mathcal{B}(D^+\to\eta^\prime e^+ \nu_e)$ obtained from previous BESIII measurements, the $\eta-\eta^\prime$ mixing angle in the quark flavor basis is determined to be $\phi_{\rm P} = (40.0\pm2.0_{\rm stat.}\pm0.6_{\rm syst.})^\circ$. Moreover, from the fits to the partial decay rates of $D_s^+ \to \eta e^+ \nu_e$ and $D_s^+ \to \eta^\prime e^+ \nu_e$, the products of the hadronic transition form factors $f_+^{\eta^{(\prime)}}(0)$ and the modulus of the $c\to s$ Cabibbo-Kobayashi-Maskawa matrix element $|V_{cs}|$ are determined by using different hadronic transition form factor parametrizations. Based on the two-parameter series expansion, the products $f^\eta_+(0)|V_{cs}| = 0.4553\pm0.0071_{\rm stat}\pm0.0061_{\rm syst}$ and $f^{\eta^\prime}_+(0)|V_{cs}| = 0.529\pm0.024_{\rm stat}\pm0.008_{\rm syst}$ are extracted. All results determined in this work supersede those measured in the previous BESIII analyses based on the 3.19 fb$^{-1}$ subsample of data at 4.178 GeV.|
|**2023-06-08**|**Bayesian Inference for $k$-Monotone Densities with Applications to Multiple Testing**|Kang Wang et.al.|[2306.05173v1](http://arxiv.org/abs/2306.05173v1)|null|Shape restriction, like monotonicity or convexity, imposed on a function of interest, such as a regression or density function, allows for its estimation without smoothness assumptions. The concept of $k$-monotonicity encompasses a family of shape restrictions, including decreasing and convex decreasing as special cases corresponding to $k=1$ and $k=2$. We consider Bayesian approaches to estimate a $k$-monotone density. By utilizing a kernel mixture representation and putting a Dirichlet process or a finite mixture prior on the mixing distribution, we show that the posterior contraction rate in the Hellinger distance is $(n/\log n)^{- k/(2k + 1)}$ for a $k$-monotone density, which is minimax optimal up to a polylogarithmic factor. When the true $k$-monotone density is a finite $J_0$-component mixture of the kernel, the contraction rate improves to the nearly parametric rate $\sqrt{(J_0 \log n)/n}$. Moreover, by putting a prior on $k$, we show that the same rates hold even when the best value of $k$ is unknown. A specific application in modeling the density of $p$-values in a large-scale multiple testing problem is considered. Simulation studies are conducted to evaluate the performance of the proposed method.|
|**2023-06-08**|**FLEdge: Benchmarking Federated Machine Learning Applications in Edge Computing Systems**|Herbert Woisetschläger et.al.|[2306.05172v1](http://arxiv.org/abs/2306.05172v1)|null|Federated Machine Learning (FL) has received considerable attention in recent years. FL benchmarks are predominantly explored in either simulated systems or data center environments, neglecting the setups of real-world systems, which are often closely linked to edge computing. We close this research gap by introducing FLEdge, a benchmark targeting FL workloads in edge computing systems. We systematically study hardware heterogeneity, energy efficiency during training, and the effect of various differential privacy levels on training in FL systems. To make this benchmark applicable to real-world scenarios, we evaluate the impact of client dropouts on state-of-the-art FL strategies with failure rates as high as 50%. FLEdge provides new insights, such as that training state-of-the-art FL workloads on older GPU-accelerated embedded devices is up to 3x more energy efficient than on modern server-grade GPUs.|
|**2023-06-08**|**Bayesian Optimization of Expensive Nested Grey-Box Functions**|Wenjie Xu et.al.|[2306.05150v1](http://arxiv.org/abs/2306.05150v1)|null|We consider the problem of optimizing a grey-box objective function, i.e., nested function composed of both black-box and white-box functions. A general formulation for such grey-box problems is given, which covers the existing grey-box optimization formulations as special cases. We then design an optimism-driven algorithm to solve it. Under certain regularity assumptions, our algorithm achieves similar regret bound as that for the standard black-box Bayesian optimization algorithm, up to a constant multiplicative term depending on the Lipschitz constants of the functions considered. We further extend our method to the constrained case and discuss several special cases. For the commonly used kernel functions, the regret bounds allow us to derive a convergence rate to the optimal solution. Experimental results show that our grey-box optimization method empirically improves the speed of finding the global optimal solution significantly, as compared to the standard black-box optimization algorithm.|
|**2023-06-08**|**Variable Radiance Field for Real-Life Category-Specifc Reconstruction from Single Image**|Kun Wang et.al.|[2306.05145v1](http://arxiv.org/abs/2306.05145v1)|null|Reconstructing category-specific objects from a single image is a challenging task that requires inferring the geometry and appearance of an object from a limited viewpoint. Existing methods typically rely on local feature retrieval based on re-projection with known camera intrinsic, which are slow and prone to distortion at viewpoints distant from the input image. In this paper, we present Variable Radiance Field (VRF), a novel framework that can efficiently reconstruct category-specific objects from a single image without known camera parameters. Our key contributions are: (1) We parameterize the geometry and appearance of the object using a multi-scale global feature extractor, which avoids frequent point-wise feature retrieval and camera dependency. We also propose a contrastive learning-based pretraining strategy to improve the feature extractor. (2) We reduce the geometric complexity of the object by learning a category template, and use hypernetworks to generate a small neural radiance field for fast and instance-specific rendering. (3) We align each training instance to the template space using a learned similarity transformation, which enables semantic-consistent learning across different objects. We evaluate our method on the CO3D dataset and show that it outperforms existing methods in terms of quality and speed. We also demonstrate its applicability to shape interpolation and object placement tasks.|
|**2023-06-08**|**Mesogeos: A multi-purpose dataset for data-driven wildfire modeling in the Mediterranean**|Spyros Kondylatos et.al.|[2306.05144v1](http://arxiv.org/abs/2306.05144v1)|[link](https://github.com/orion-ai-lab/mesogeos)|We introduce Mesogeos, a large-scale multi-purpose dataset for wildfire modeling in the Mediterranean. Mesogeos integrates variables representing wildfire drivers (meteorology, vegetation, human activity) and historical records of wildfire ignitions and burned areas for 17 years (2006-2022). It is designed as a cloud-friendly spatio-temporal dataset, namely a datacube, harmonizing all variables in a grid of 1km x 1km x 1-day resolution. The datacube structure offers opportunities to assess machine learning (ML) usage in various wildfire modeling tasks. We extract two ML-ready datasets that establish distinct tracks to demonstrate this potential: (1) short-term wildfire danger forecasting and (2) final burned area estimation given the point of ignition. We define appropriate metrics and baselines to evaluate the performance of models in each track. By publishing the datacube, along with the code to create the ML datasets and models, we encourage the community to foster the implementation of additional tracks for mitigating the increasing threat of wildfires in the Mediterranean.|
|**2023-06-08**|**A Two-dimensional Spatial Optimization Framework for Vehicle Powertrain Systems**|Jorn van Kampen et.al.|[2306.05140v1](http://arxiv.org/abs/2306.05140v1)|null|This paper presents a modeling framework to optimize the two-dimensional placement of powertrain elements inside the vehicle, explicitly accounting for the rotation, relative placement and alignment. Specifically, we first capture the multi-level nature of the system mathematically, and construct a model that captures different powertrain component orientations. Second, we include the relative element placement as variables in the model and derive alignment constraints for both child components and parent subsystems to automatically connect mechanical ports. Finally, we showcase our framework on a four-wheel driven electric vehicle. Our results demonstrate that our framework is capable of efficiently generating system design solutions in a fully automated manner, only using basic component properties.|

## heart rate

### heart rate
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**Classical simulations of noisy variational quantum circuits**|Enrico Fontana et.al.|[2306.05400v1](http://arxiv.org/abs/2306.05400v1)|null|Noise detrimentally affects quantum computations so that they not only become less accurate but also easier to simulate classically as systems scale up. We construct a classical simulation algorithm, LOWESA (low weight efficient simulation algorithm), for estimating expectation values of noisy parameterised quantum circuits. It combines previous results on spectral analysis of parameterised circuits with Pauli back-propagation and recent ideas for simulations of noisy random circuits. We show, under some conditions on the circuits and mild assumptions on the noise, that LOWESA gives an efficient, polynomial algorithm in the number of qubits (and depth), with approximation error that vanishes exponentially in the physical error rate and a controllable cut-off parameter. We also discuss the practical limitations of the method for circuit classes with correlated parameters and its scaling with decreasing error rates.|
|**2023-06-08**|**HQ-50K: A Large-scale, High-quality Dataset for Image Restoration**|Qinhong Yang et.al.|[2306.05390v1](http://arxiv.org/abs/2306.05390v1)|[link](https://github.com/littleyaang/hq-50k)|This paper introduces a new large-scale image restoration dataset, called HQ-50K, which contains 50,000 high-quality images with rich texture details and semantic diversity. We analyze existing image restoration datasets from five different perspectives, including data scale, resolution, compression rates, texture details, and semantic coverage. However, we find that all of these datasets are deficient in some aspects. In contrast, HQ-50K considers all of these five aspects during the data curation process and meets all requirements. We also present a new Degradation-Aware Mixture of Expert (DAMoE) model, which enables a single model to handle multiple corruption types and unknown levels. Our extensive experiments demonstrate that HQ-50K consistently improves the performance on various image restoration tasks, such as super-resolution, denoising, dejpeg, and deraining. Furthermore, our proposed DAMoE, trained on our \dataset, outperforms existing state-of-the-art unified models designed for multiple restoration tasks and levels. The dataset and code are available at \url{https://github.com/littleYaang/HQ-50K}.|
|**2023-06-08**|**Utterance Emotion Dynamics in Children's Poems: Emotional Changes Across Age**|Daniela Teodorescu et.al.|[2306.05387v1](http://arxiv.org/abs/2306.05387v1)|null|Emerging psychopathology studies are showing that patterns of changes in emotional state -- emotion dynamics -- are associated with overall well-being and mental health. More recently, there has been some work in tracking emotion dynamics through one's utterances, allowing for data to be collected on a larger scale across time and people. However, several questions about how emotion dynamics change with age, especially in children, and when determined through children's writing, remain unanswered. In this work, we use both a lexicon and a machine learning based approach to quantify characteristics of emotion dynamics determined from poems written by children of various ages. We show that both approaches point to similar trends: consistent increasing intensities for some emotions (e.g., anger, fear, joy, sadness, arousal, and dominance) with age and a consistent decreasing valence with age. We also find increasing emotional variability, rise rates (i.e., emotional reactivity), and recovery rates (i.e., emotional regulation) with age. These results act as a useful baselines for further research in how patterns of emotions expressed by children change with age, and their association with mental health.|
|**2023-06-08**|**Rate Forecaster based Energy Aware Band Assignment in Multiband Networks**|Brijesh Soni et.al.|[2306.05369v1](http://arxiv.org/abs/2306.05369v1)|null|The high frequency communication bands (mmWave and sub-THz) promise tremendous data rates, however, they also have very high power consumption which is particularly significant for battery-power-limited user-equipment (UE). In this context, we design an energy aware band assignment system which reduces the power consumption while also achieving a target sum rate of M in T time-slots. We do this by using 1) Rate forecaster(s); 2) Channel forecaster(s) which forecasts T direct multistep ahead using a stacked (long short term memory) LSTM architecture. We propose an iterative rate updating algorithm which updates the target rate based on current rate and future predicted rates in a frame. The proposed approach is validated on the publicly available `DeepMIMO' dataset. Research findings shows that the rate forecaster based approach performs better than the channel forecaster. Furthermore, LSTM based predictions outperforms well celebrated Transformer predictions in terms of NRMSE and NMAE. Research findings reveals that the power consumption with this approach is ~ 300 mW lower compared to a greedy band assignment at a 1.5Gb/s target rate.|
|**2023-06-08**|**Ordinal Potential-based Player Rating**|Nelson Vadori et.al.|[2306.05366v1](http://arxiv.org/abs/2306.05366v1)|null|A two-player symmetric zero-sum game is transitive if for any pure strategies $x$, $y$, $z$, if $x$ is better than $y$, and $y$ is better than $z$, then $x$ is better than $z$. It was recently observed that the Elo rating fails at preserving transitive relations among strategies and therefore cannot correctly extract the transitive component of a game. Our first contribution is to show that the Elo rating actually does preserve transitivity when computed in the right space. Precisely, using a suitable invertible mapping $\varphi$, we first apply $\varphi$ to the game, then compute Elo ratings, then go back to the original space by applying $\varphi^{-1}$. We provide a characterization of transitive games as a weak variant of ordinal potential games with additively separable potential functions. Leveraging this insight, we introduce the concept of transitivity order, the minimum number of invertible mappings required to transform the payoff of a transitive game into (differences of) its potential function. The transitivity order is a tool to classify transitive games, with Elo games being an example of transitive games of order one. Most real-world games have both transitive and non-transitive (cyclic) components, and we use our analysis of transitivity to extract the transitive (potential) component of an arbitrary game. We link transitivity to the known concept of sign-rank: transitive games have sign-rank two; arbitrary games may have higher sign-rank. Using a neural network-based architecture, we learn a decomposition of an arbitrary game into transitive and cyclic components that prioritises capturing the sign pattern of the game. In particular, a transitive game always has just one component in its decomposition, the potential component. We provide a comprehensive evaluation of our methodology using both toy examples and empirical data from real-world games.|
|**2023-06-08**|**A Data-Driven Approach to Positioning Grab Bars in the Sagittal Plane for Elderly Persons**|Roberto Bolli Jr. et.al.|[2306.05343v1](http://arxiv.org/abs/2306.05343v1)|null|The placement of grab bars for elderly users is based largely on ADA building codes and does not reflect the large differences in height, mobility, and muscle power between individual persons. The goal of this study is to see if there are any correlations between an elderly user's preferred handlebar pose and various demographic indicators, self-rated mobility for tasks requiring postural change, and biomechanical markers. For simplicity, we consider only the case where the handlebar is positioned directly in front of the user, as this confines the relevant body kinematics to a 2D sagittal plane. Previous eldercare devices have been constructed to position a handlebar in various poses in space. Our work augments these devices and adds to the body of knowledge by assessing how the handlebar should be positioned based on data on actual elderly people instead of simulations.|
|**2023-06-08**|**Spontaneous Self-Constraint in Active Nematic Flows**|Louise C. Head et.al.|[2306.05328v1](http://arxiv.org/abs/2306.05328v1)|null|Active processes drive and guide biological dynamics across scales -- from subcellular cytoskeletal remodelling, through tissue development in embryogenesis, to population-level bacterial colonies expansion. In each of these, biological functionality requires collective flows to occur while self-organized structures are protected; however, the mechanisms by which active flows can spontaneously constrain their dynamics to preserve structure have not previously been explained. By studying collective flows and defect dynamics in active nematic films, we demonstrate the existence of a self-constraint -- a two-way, spontaneously arising relationship between activity-driven isosurfaces of flow boundaries and mesoscale nematic structures. Our results show that self-motile defects are tightly constrained to viscometric surfaces -- contours along which vorticity and strain-rate balance. This in turn reveals that self-motile defects break mirror symmetry when they move along a single viscometric surface, in contrast with expectations. This is explained by an interdependence between viscometric surfaces and bend walls -- elongated narrow kinks in the orientation field. Although we focus on extensile nematic films, numerical results show the constraint holds whenever activity leads to motile half-charge defects. This mesoscale cross-field self-constraint offers a new framework for tackling complex 3D active turbulence, designing dynamic control into biomimetic materials, and understanding how biological systems can employ active stress for dynamic self-organization.|
|**2023-06-08**|**Perching by hugging: an initial feasibility study**|William Stewart et.al.|[2306.05324v1](http://arxiv.org/abs/2306.05324v1)|null|Current UAVs capable of perching require added structure and mechanisms to accomplish this. These take the form of hooks, claws, needles, etc which add weight and usually drag. We propose in this paper the dual use of structures already on the vehicle to enable perching, thus reducing the weight and drag cost associated with perching UAVs. We propose a wing design capable of passively wrapping around a vertical pole to perch. We experimentally investigate the feasibility of the design, presenting results on minimum required perching speeds as well as the effect of weight distribution on the success rate of the wing wrapping. Finally, we comment on design requirements for holding onto the pole based on our findings.|
|**2023-06-08**|**Real-time whole-heart electromechanical simulations using Latent Neural Ordinary Differential Equations**|Matteo Salvador et.al.|[2306.05321v1](http://arxiv.org/abs/2306.05321v1)|null|Cardiac digital twins provide a physics and physiology informed framework to deliver predictive and personalized medicine. However, high-fidelity multi-scale cardiac models remain a barrier to adoption due to their extensive computational costs and the high number of model evaluations needed for patient-specific personalization. Artificial Intelligence-based methods can make the creation of fast and accurate whole-heart digital twins feasible. In this work, we use Latent Neural Ordinary Differential Equations (LNODEs) to learn the temporal pressure-volume dynamics of a heart failure patient. Our surrogate model based on LNODEs is trained from 400 3D-0D whole-heart closed-loop electromechanical simulations while accounting for 43 model parameters, describing single cell through to whole organ and cardiovascular hemodynamics. The trained LNODEs provides a compact and efficient representation of the 3D-0D model in a latent space by means of a feedforward fully-connected Artificial Neural Network that retains 3 hidden layers with 13 neurons per layer and allows for 300x real-time numerical simulations of the cardiac function on a single processor of a standard laptop. This surrogate model is employed to perform global sensitivity analysis and robust parameter estimation with uncertainty quantification in 3 hours of computations, still on a single processor. We match pressure and volume time traces unseen by the LNODEs during the training phase and we calibrate 4 to 11 model parameters while also providing their posterior distribution. This paper introduces the most advanced surrogate model of cardiac function available in the literature and opens new important venues for parameter calibration in cardiac digital twins.|
|**2023-06-08**|**Large-scale adaptive multiple testing for sequential data controlling false discovery and nondiscovery rates**|Rahul Roy et.al.|[2306.05315v1](http://arxiv.org/abs/2306.05315v1)|null|In modern scientific experiments, we frequently encounter data that have large dimensions, and in some experiments, such high dimensional data arrive sequentially rather than full data being available all at a time. We develop multiple testing procedures with simultaneous control of false discovery and nondiscovery rates when $m$-variate data vectors $\mathbf{X}_1, \mathbf{X}_2, \dots$ are observed sequentially or in groups and each coordinate of these vectors leads to a hypothesis testing. Existing multiple testing methods for sequential data uses fixed stopping boundaries that do not depend on sample size, and hence, are quite conservative when the number of hypotheses $m$ is large. We propose sequential tests based on adaptive stopping boundaries that ensure shrinkage of the continue sampling region as the sample size increases. Under minimal assumptions on the data sequence, we first develop a test based on an oracle test statistic such that both false discovery rate (FDR) and false nondiscovery rate (FNR) are nearly equal to some prefixed levels with strong control. Under a two-group mixture model assumption, we propose a data-driven stopping and decision rule based on local false discovery rate statistic that mimics the oracle rule and guarantees simultaneous control of FDR and FNR asymptotically as $m$ tends to infinity. Both the oracle and the data-driven stopping times are shown to be finite (i.e., proper) with probability 1 for all finite $m$ and converge to a finite constant as $m$ grows to infinity. Further, we compare the data-driven test with the existing gap rule proposed in He and Bartroff (2021) and show that the ratio of the expected sample sizes of our method and the gap rule tends to zero as $m$ goes to infinity. Extensive analysis of simulated datasets as well as some real datasets illustrate the superiority of the proposed tests over some existing methods.|
|**2023-06-08**|**Mode-locked laser in nanophotonic lithium niobate**|Qiushi Guo et.al.|[2306.05314v1](http://arxiv.org/abs/2306.05314v1)|null|Mode-locked lasers (MLLs) have enabled ultrafast sciences and technologies by generating ultrashort pulses with peak powers substantially exceeding their average powers. Recently, tremendous efforts have been focused on realizing integrated MLLs not only to address the challenges associated with their size and power demand, but also to enable transforming the ultrafast technologies into nanophotonic chips, and ultimately to unlock their potential for a plethora of applications. However, till now the prospect of integrated MLLs driving ultrafast nanophotonic circuits has remained elusive because of their typically low peak powers, lack of controllability, and challenges with integration with appropriate nanophotonic platforms. Here, we overcome these limitations by demonstrating an electrically-pumped actively MLL in nanophotonic lithium niobate based on its hybrid integration with a III-V semiconductor optical amplifier. Our MLL generates $\sim$4.8 ps optical pulses around 1065 nm at a repetition rate of $\sim$10 GHz, with pulse energy exceeding 2.6 pJ and a high peak power beyond 0.5 W. We show that both the repetition rate and the carrier-envelope-offset of the resulting frequency comb can be flexibly controlled in a wide range using the RF driving frequency and the pump current, paving the way for fully-stabilized on-chip frequency combs in nanophotonics. Our work marks an important step toward fully-integrated nonlinear and ultrafast photonic systems in nanophotonic lithium niobate.|
|**2023-06-08**|**Chiral EFT calculation of neutrino reactions in warm neutron-rich matter**|Eunkyoung Shin et.al.|[2306.05280v1](http://arxiv.org/abs/2306.05280v1)|null|Neutrino scattering and absorption rates of relevance to supernovae and neutron star mergers are obtained from nuclear matter dynamical structure functions that encode many-body effects from nuclear mean fields and correlations. We employ nuclear interactions from chiral effective field theory to calculate the density, spin, isospin, and spin-isospin response functions of warm beta-equilibrium nuclear matter. We include corrections to the single-particle energies in the mean field approximation as well as vertex corrections resummed in the random phase approximation (RPA), including, for the first time, both direct and exchange diagrams. We find that correlations included through the RPA redistribute the strength of the response to higher energy for neutrino absorption and lower energy for antineutrino absorption. This tends to suppress the absorption rate of electron neutrinos across all relevant energy scales. In contrast, the inclusion of RPA correlations enhances the electron antineutrino absorption rate at low energy and supresses the rate at high energy. These effects are especially important at high-density and in the vicinity of the neutrino decoupling region. Implications for heavy element nucleosynthesis, electromagnetic signatures of compact object mergers, supernova dynamics, and neutrino detection from galactic supernovae are discussed briefly.|
|**2023-06-08**|**Language-specific Acoustic Boundary Learning for Mandarin-English Code-switching Speech Recognition**|Zhiyun Fan et.al.|[2306.05279v1](http://arxiv.org/abs/2306.05279v1)|null|Code-switching speech recognition (CSSR) transcribes speech that switches between multiple languages or dialects within a single sentence. The main challenge in this task is that different languages often have similar pronunciations, making it difficult for models to distinguish between them. In this paper, we propose a method for solving the CSSR task from the perspective of language-specific acoustic boundary learning. We introduce language-specific weight estimators (LSWE) to model acoustic boundary learning in different languages separately. Additionally, a non-autoregressive (NAR) decoder and a language change detection (LCD) module are employed to assist in training. Evaluated on the SEAME corpus, our method achieves a state-of-the-art mixed error rate (MER) of 16.29% and 22.81% on the test_man and test_sge sets. We also demonstrate the effectiveness of our method on a 9000-hour in-house meeting code-switching dataset, where our method achieves a relatively 7.9% MER reduction.|
|**2023-06-08**|**Large deviations of return times and related entropy estimators on shift spaces**|Noé Cuneo et.al.|[2306.05277v1](http://arxiv.org/abs/2306.05277v1)|null|We prove the large deviation principle for several entropy and cross entropy estimators based on return times and waiting times on shift spaces over finite alphabets. In the case of standard return times, we obtain a nonconvex large-deviation rate function. We consider shift-invariant probability measures satisfying some decoupling conditions which imply no form of mixing nor ergodicity. We establish precise relations between the rate functions of the different estimators, and between these rate functions and the corresponding pressures, one of which is the R\'enyi entropy function. The results apply in particular to irreducible Markov chains, equilibrium measures for Bowen-regular potentials, g-measures, invariant Gibbs states for summable interactions in statistical mechanics, and also to probability measures that may be far from Gibbsian, including some hidden Markov models and repeated quantum measurement processes.|
|**2023-06-08**|**Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models**|Tianzhe Chu et.al.|[2306.05272v1](http://arxiv.org/abs/2306.05272v1)|[link](https://github.com/leslietrue/cpp)|The advent of large pre-trained models has brought about a paradigm shift in both visual representation learning and natural language processing. However, clustering unlabeled images, as a fundamental and classic machine learning problem, still lacks effective solution, particularly for large-scale datasets. In this paper, we propose a novel image clustering pipeline that leverages the powerful feature representation of large pre-trained models such as CLIP and cluster images effectively and efficiently at scale. We show that the pre-trained features are significantly more structured by further optimizing the rate reduction objective. The resulting features may significantly improve the clustering accuracy, e.g., from 57\% to 66\% on ImageNet-1k. Furthermore, by leveraging CLIP's image-text binding, we show how the new clustering method leads to a simple yet effective self-labeling algorithm that successfully works on unlabeled large datasets such as MS-COCO and LAION-Aesthetics. We will release the code in https://github.com/LeslieTrue/CPP.|
|**2023-06-08**|**Toward more accurate and generalizable brain deformation estimators for traumatic brain injury detection with unsupervised domain adaptation**|Xianghao Zhan et.al.|[2306.05255v1](http://arxiv.org/abs/2306.05255v1)|[link](https://github.com/xzhan96-stf/drca-mlhm)|Machine learning head models (MLHMs) are developed to estimate brain deformation for early detection of traumatic brain injury (TBI). However, the overfitting to simulated impacts and the lack of generalizability caused by distributional shift of different head impact datasets hinders the broad clinical applications of current MLHMs. We propose brain deformation estimators that integrates unsupervised domain adaptation with a deep neural network to predict whole-brain maximum principal strain (MPS) and MPS rate (MPSR). With 12,780 simulated head impacts, we performed unsupervised domain adaptation on on-field head impacts from 302 college football (CF) impacts and 457 mixed martial arts (MMA) impacts using domain regularized component analysis (DRCA) and cycle-GAN-based methods. The new model improved the MPS/MPSR estimation accuracy, with the DRCA method significantly outperforming other domain adaptation methods in prediction accuracy (p<0.001): MPS RMSE: 0.027 (CF) and 0.037 (MMA); MPSR RMSE: 7.159 (CF) and 13.022 (MMA). On another two hold-out test sets with 195 college football impacts and 260 boxing impacts, the DRCA model significantly outperformed the baseline model without domain adaptation in MPS and MPSR estimation accuracy (p<0.001). The DRCA domain adaptation reduces the MPS/MPSR estimation error to be well below TBI thresholds, enabling accurate brain deformation estimation to detect TBI in future clinical applications.|
|**2023-06-08**|**Matching Latent Encoding for Audio-Text based Keyword Spotting**|Kumari Nishu et.al.|[2306.05245v1](http://arxiv.org/abs/2306.05245v1)|null|Using audio and text embeddings jointly for Keyword Spotting (KWS) has shown high-quality results, but the key challenge of how to semantically align two embeddings for multi-word keywords of different sequence lengths remains largely unsolved. In this paper, we propose an audio-text-based end-to-end model architecture for flexible keyword spotting (KWS), which builds upon learned audio and text embeddings. Our architecture uses a novel dynamic programming-based algorithm, Dynamic Sequence Partitioning (DSP), to optimally partition the audio sequence into the same length as the word-based text sequence using the monotonic alignment of spoken content. Our proposed model consists of an encoder block to get audio and text embeddings, a projector block to project individual embeddings to a common latent space, and an audio-text aligner containing a novel DSP algorithm, which aligns the audio and text embeddings to determine if the spoken content is the same as the text. Experimental results show that our DSP is more effective than other partitioning schemes, and the proposed architecture outperformed the state-of-the-art results on the public dataset in terms of Area Under the ROC Curve (AUC) and Equal-Error-Rate (EER) by 14.4 % and 28.9%, respectively.|
|**2023-06-08**|**Precision Measurements of $D_s^+ \to ηe^+ ν_e$ and $D_s^+ \to η^\prime e^+ ν_e$**|BESIII Collaboration et.al.|[2306.05194v1](http://arxiv.org/abs/2306.05194v1)|null|Precision measurements of the semileptonic decays $D_s^+ \to \eta e^+ \nu_e$ and $D_s^+ \to \eta^\prime e^+ \nu_e$ are performed using 7.33\,fb$^{-1}$ of $e^+e^-$ collision data collected at center-of-mass energies between 4.128 and 4.226 GeV with the BESIII detector. The branching fractions obtained are $\mathcal{B}(D_s^+ \to \eta e^{+} \nu_e)$ = $(2.251\pm0.039_{\rm stat.}\pm 0.051_{\rm syst.})\%$ and $\mathcal{B}(D_s^+ \to \eta^{\prime} e^{+} \nu_e)$ = $(0.810\pm0.038_{\rm stat.}\pm 0.024_{\rm syst.})\%$. Combining these results with the $\mathcal{B}(D^+\to\eta e^+ \nu_e)$ and $\mathcal{B}(D^+\to\eta^\prime e^+ \nu_e)$ obtained from previous BESIII measurements, the $\eta-\eta^\prime$ mixing angle in the quark flavor basis is determined to be $\phi_{\rm P} = (40.0\pm2.0_{\rm stat.}\pm0.6_{\rm syst.})^\circ$. Moreover, from the fits to the partial decay rates of $D_s^+ \to \eta e^+ \nu_e$ and $D_s^+ \to \eta^\prime e^+ \nu_e$, the products of the hadronic transition form factors $f_+^{\eta^{(\prime)}}(0)$ and the modulus of the $c\to s$ Cabibbo-Kobayashi-Maskawa matrix element $|V_{cs}|$ are determined by using different hadronic transition form factor parametrizations. Based on the two-parameter series expansion, the products $f^\eta_+(0)|V_{cs}| = 0.4553\pm0.0071_{\rm stat}\pm0.0061_{\rm syst}$ and $f^{\eta^\prime}_+(0)|V_{cs}| = 0.529\pm0.024_{\rm stat}\pm0.008_{\rm syst}$ are extracted. All results determined in this work supersede those measured in the previous BESIII analyses based on the 3.19 fb$^{-1}$ subsample of data at 4.178 GeV.|
|**2023-06-08**|**Bayesian Inference for $k$-Monotone Densities with Applications to Multiple Testing**|Kang Wang et.al.|[2306.05173v1](http://arxiv.org/abs/2306.05173v1)|null|Shape restriction, like monotonicity or convexity, imposed on a function of interest, such as a regression or density function, allows for its estimation without smoothness assumptions. The concept of $k$-monotonicity encompasses a family of shape restrictions, including decreasing and convex decreasing as special cases corresponding to $k=1$ and $k=2$. We consider Bayesian approaches to estimate a $k$-monotone density. By utilizing a kernel mixture representation and putting a Dirichlet process or a finite mixture prior on the mixing distribution, we show that the posterior contraction rate in the Hellinger distance is $(n/\log n)^{- k/(2k + 1)}$ for a $k$-monotone density, which is minimax optimal up to a polylogarithmic factor. When the true $k$-monotone density is a finite $J_0$-component mixture of the kernel, the contraction rate improves to the nearly parametric rate $\sqrt{(J_0 \log n)/n}$. Moreover, by putting a prior on $k$, we show that the same rates hold even when the best value of $k$ is unknown. A specific application in modeling the density of $p$-values in a large-scale multiple testing problem is considered. Simulation studies are conducted to evaluate the performance of the proposed method.|
|**2023-06-08**|**FLEdge: Benchmarking Federated Machine Learning Applications in Edge Computing Systems**|Herbert Woisetschläger et.al.|[2306.05172v1](http://arxiv.org/abs/2306.05172v1)|null|Federated Machine Learning (FL) has received considerable attention in recent years. FL benchmarks are predominantly explored in either simulated systems or data center environments, neglecting the setups of real-world systems, which are often closely linked to edge computing. We close this research gap by introducing FLEdge, a benchmark targeting FL workloads in edge computing systems. We systematically study hardware heterogeneity, energy efficiency during training, and the effect of various differential privacy levels on training in FL systems. To make this benchmark applicable to real-world scenarios, we evaluate the impact of client dropouts on state-of-the-art FL strategies with failure rates as high as 50%. FLEdge provides new insights, such as that training state-of-the-art FL workloads on older GPU-accelerated embedded devices is up to 3x more energy efficient than on modern server-grade GPUs.|
|**2023-06-08**|**Bayesian Optimization of Expensive Nested Grey-Box Functions**|Wenjie Xu et.al.|[2306.05150v1](http://arxiv.org/abs/2306.05150v1)|null|We consider the problem of optimizing a grey-box objective function, i.e., nested function composed of both black-box and white-box functions. A general formulation for such grey-box problems is given, which covers the existing grey-box optimization formulations as special cases. We then design an optimism-driven algorithm to solve it. Under certain regularity assumptions, our algorithm achieves similar regret bound as that for the standard black-box Bayesian optimization algorithm, up to a constant multiplicative term depending on the Lipschitz constants of the functions considered. We further extend our method to the constrained case and discuss several special cases. For the commonly used kernel functions, the regret bounds allow us to derive a convergence rate to the optimal solution. Experimental results show that our grey-box optimization method empirically improves the speed of finding the global optimal solution significantly, as compared to the standard black-box optimization algorithm.|
|**2023-06-08**|**Orthogonal Sampling based Broad-Band Signal Generation with Low-Bandwidth Electronics**|Mohamed I. Hosni et.al.|[2306.05125v1](http://arxiv.org/abs/2306.05125v1)|null|High-bandwidth signals are needed in many applications like radar, sensing, measurement and communications. Especially in optical networks, the sampling rate and analog bandwidth of digital-to-analog converters (DACs) is a bottleneck for further increasing data rates. To circumvent the sampling rate and bandwidth problem of electronic DACs, we demonstrate the generation of wide-band signals with low-bandwidth electronics. This generation is based on orthogonal sampling with sinc-pulse sequences in N parallel branches. The method not only reduces the sampling rate and bandwidth, at the same time the effective number of bits (ENOB) is improved, dramatically reducing the requirements on the electronic signal processing. In proof of concept experiments the generation of analog signals, as well as Nyquist shaped and normal data will be shown. In simulations we investigate the performance of 60 GHz data generation by 20 and 12 GHz electronics. The method can easily be integrated together with already existing electronic DAC designs and would be of great interest for all high-bandwidth applications.|
|**2023-06-08**|**Stabilizing Discontinuous Galerkin Methods Using Dafermos' Entropy Rate Criterion: II -- Systems of Conservation Laws and Entropy Inequality Predictors**|Simon-Christian Klein et.al.|[2306.05124v1](http://arxiv.org/abs/2306.05124v1)|null|A novel approach for the stabilization of the Discontinuous Galerkin method based on the Dafermos entropy rate crition is presented. First, estimates for the maximal possible entropy dissipation rate of a weak solution are derived. Second, families of conservative Hilbert-Schmidt operators are identified to dissipate entropy. Steering these operators using the bounds on the entropy dissipation results in high-order accurate shock-capturing DG schemes for the Euler equations, satisfying the entropy rate criterion and an entropy inequality.|
|**2023-06-08**|**FheFL: Fully Homomorphic Encryption Friendly Privacy-Preserving Federated Learning with Byzantine Users**|Yogachandran Rahulamathavan et.al.|[2306.05112v1](http://arxiv.org/abs/2306.05112v1)|null|The federated learning (FL) technique was initially developed to mitigate data privacy issues that can arise in the traditional machine learning paradigm. While FL ensures that a user's data always remain with the user, the gradients of the locally trained models must be communicated with the centralized server to build the global model. This results in privacy leakage, where the server can infer private information of the users' data from the shared gradients. To mitigate this flaw, the next-generation FL architectures proposed encryption and anonymization techniques to protect the model updates from the server. However, this approach creates other challenges, such as a malicious user might sabotage the global model by sharing false gradients. Since the gradients are encrypted, the server is unable to identify and eliminate rogue users which would protect the global model. Therefore, to mitigate both attacks, this paper proposes a novel fully homomorphic encryption (FHE) based scheme suitable for FL. We modify the one-to-one single-key Cheon-Kim-Kim-Song (CKKS)-based FHE scheme into a distributed multi-key additive homomorphic encryption scheme that supports model aggregation in FL. We employ a novel aggregation scheme within the encrypted domain, utilizing users' non-poisoning rates, to effectively address data poisoning attacks while ensuring privacy is preserved by the proposed encryption scheme. Rigorous security, privacy, convergence, and experimental analyses have been provided to show that FheFL is novel, secure, and private, and achieves comparable accuracy at reasonable computational cost.|
|**2023-06-08**|**Re-aligning Shadow Models can Improve White-box Membership Inference Attacks**|Ana-Maria Cretu et.al.|[2306.05093v1](http://arxiv.org/abs/2306.05093v1)|null|Machine learning models have been shown to leak sensitive information about their training datasets. As models are being increasingly used, on devices, to automate tasks and power new applications, there have been concerns that such white-box access to its parameters, as opposed to the black-box setting which only provides query access to the model, increases the attack surface. Directly extending the shadow modelling technique from the black-box to the white-box setting has been shown, in general, not to perform better than black-box only attacks. A key reason is misalignment, a known characteristic of deep neural networks. We here present the first systematic analysis of the causes of misalignment in shadow models and show the use of a different weight initialisation to be the main cause of shadow model misalignment. Second, we extend several re-alignment techniques, previously developed in the model fusion literature, to the shadow modelling context, where the goal is to re-align the layers of a shadow model to those of the target model.We show re-alignment techniques to significantly reduce the measured misalignment between the target and shadow models. Finally, we perform a comprehensive evaluation of white-box membership inference attacks (MIA). Our analysis reveals that (1) MIAs suffer from misalignment between shadow models, but that (2) re-aligning the shadow models improves, sometimes significantly, MIA performance. On the CIFAR10 dataset with a false positive rate of 1\%, white-box MIA using re-aligned shadow models improves the true positive rate by 4.5\%.Taken together, our results highlight that on-device deployment increase the attack surface and that the newly available information can be used by an attacker.|
|**2023-06-08**|**Selenium and the role of defects for photovoltaic applications**|Hadeel Moustafa et.al.|[2306.05092v1](http://arxiv.org/abs/2306.05092v1)|null|We present first principles calculations of the electronic properties of trigonal selenium with emphasis on photovoltaic applications. The band gap and optical absorption spectrum of pristine selenium is calculated from many-body perturbation theory yielding excellent agreement with experiments. We then investigate the role of intrinsic as well as extrinsic defects and estimate the equilibrium concentrations resulting from realistic synthesis conditions. The intrinsic defects are dominated by vacancies, which act as acceptor levels and implies $p$-doping in agreement with previous predictions and measurements, and we show that these do not give rise to significant non-radiative recombination. The charge balance remains dominated by vacancies when extrinsic defects are included, but these may give rise to sizable non-radiative recombination rates, which could severely limit the performance of selenium based solar cells. Our results thus imply that the pollution by external elements is a decisive factor for the photovoltaic efficiency, which will be of crucial importance when considering synthesis conditions for any type of device engineering.|
|**2023-06-08**|**Vanishing of long time average p-enstrophy dissipation rate in the inviscid limit of the 2D damped Navier-Stokes equations**|Raphael Wagner et.al.|[2306.05081v1](http://arxiv.org/abs/2306.05081v1)|null|In 2007, Constantin and Ramos proved a result on the vanishing long time average enstrophy dissipation rate in the inviscid limit of the 2D damped Navier-Stokes equations. In this work, we prove a generalization of this for the p-enstrophy, sequences of distributions of initial data and sequences of strongly converging right-hand sides. We simplify their approach by working with invariant measures on the global attractors which can be characterized via bounded complete solution trajectories. Then, working on the level of trajectories allows us to directly employ some recent results on strong convergence of the vorticity in the inviscid limit.|
|**2023-06-08**|**Neuro-Symbolic Approaches for Context-Aware Human Activity Recognition**|Luca Arrotta et.al.|[2306.05058v1](http://arxiv.org/abs/2306.05058v1)|null|Deep Learning models are a standard solution for sensor-based Human Activity Recognition (HAR), but their deployment is often limited by labeled data scarcity and models' opacity. Neuro-Symbolic AI (NeSy) provides an interesting research direction to mitigate these issues by infusing knowledge about context information into HAR deep learning classifiers. However, existing NeSy methods for context-aware HAR require computationally expensive symbolic reasoners during classification, making them less suitable for deployment on resource-constrained devices (e.g., mobile devices). Additionally, NeSy approaches for context-aware HAR have never been evaluated on in-the-wild datasets, and their generalization capabilities in real-world scenarios are questionable. In this work, we propose a novel approach based on a semantic loss function that infuses knowledge constraints in the HAR model during the training phase, avoiding symbolic reasoning during classification. Our results on scripted and in-the-wild datasets show the impact of different semantic loss functions in outperforming a purely data-driven model. We also compare our solution with existing NeSy methods and analyze each approach's strengths and weaknesses. Our semantic loss remains the only NeSy solution that can be deployed as a single DNN without the need for symbolic reasoning modules, reaching recognition rates close (and better in some cases) to existing approaches.|
|**2023-06-08**|**Impact of pore-scale chaotic mixing on Darcy-scale reaction rates**|Satoshi Izumoto et.al.|[2306.05018v1](http://arxiv.org/abs/2306.05018v1)|null|Prediction of reactive transport in porous media remains challenging when pore scale incomplete mixing is at play. Previous experimental studies investigated chemical reactions in porous media by visualizing reaction product or reactants mostly in uniform flow. However, the local reaction rate, which is necessary to infer mechanisms of reaction in pore space, could not be obtained without considering transport of reaction products and reactants. Thus, the interpretation remained elusive. We visualized the reaction rate field using chemiluminescnece within index-matched 3D porous media under zero acceleration and constant acceleration flow fields to investigate how pore scale chaotic mixing and Darcy scale fluid acceleration rectify reactive transport. We found that the reaction rate kept increasing from upstream to downstream in constant acceleration field, whereas it increased only at the upstream zone in zero acceleration field. The ratio of dispersion rate and size of the mixing interface determined such an effect of acceleration. Moreover, the experimental results showed stronger dependency of reaction rate on velocity compared to the numerical simulations that assume complete mixing in pore space. To explain this, we suggested the mechanistic model that includes the pore scale folding of lamellae due to chaotic mixing and the pore scale concentration gradients against compression. Such a pore scale mechanism was consistent with the experimentally observed change in reaction rate over the space. These results give new insights on underlying mechanisms of reactive transport in porous media.|
|**2023-06-08**|**Progression Cognition Reinforcement Learning with Prioritized Experience for Multi-Vehicle Pursuit**|Xinhang Li et.al.|[2306.05016v1](http://arxiv.org/abs/2306.05016v1)|[link](https://github.com/bupt-antlab/pepcrl-mvp)|Multi-vehicle pursuit (MVP) such as autonomous police vehicles pursuing suspects is important but very challenging due to its mission and safety critical nature. While multi-agent reinforcement learning (MARL) algorithms have been proposed for MVP problem in structured grid-pattern roads, the existing algorithms use randomly training samples in centralized learning, which leads to homogeneous agents showing low collaboration performance. For the more challenging problem of pursuing multiple evading vehicles, these algorithms typically select a fixed target evading vehicle for pursuing vehicles without considering dynamic traffic situation, which significantly reduces pursuing success rate. To address the above problems, this paper proposes a Progression Cognition Reinforcement Learning with Prioritized Experience for MVP (PEPCRL-MVP) in urban multi-intersection dynamic traffic scenes. PEPCRL-MVP uses a prioritization network to assess the transitions in the global experience replay buffer according to the parameters of each MARL agent. With the personalized and prioritized experience set selected via the prioritization network, diversity is introduced to the learning process of MARL, which can improve collaboration and task related performance. Furthermore, PEPCRL-MVP employs an attention module to extract critical features from complex urban traffic environments. These features are used to develop progression cognition method to adaptively group pursuing vehicles. Each group efficiently target one evading vehicle in dynamic driving environments. Extensive experiments conducted with a simulator over unstructured roads of an urban area show that PEPCRL-MVP is superior to other state-of-the-art methods. Specifically, PEPCRL-MVP improves pursuing efficiency by 3.95% over TD3-DMAP and its success rate is 34.78% higher than that of MADDPG. Codes are open sourced.|
